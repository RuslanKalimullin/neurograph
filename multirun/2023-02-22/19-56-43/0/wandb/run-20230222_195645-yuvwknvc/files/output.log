[2023-02-22 19:56:46,514][root][INFO] - Model architecture:
baseGNN(
  (convs): ModuleList(
    (0): Sequential(
      (0): GCNConv(116, 32)
      (1): LeakyReLU(negative_slope=0.2)
      (2): Dropout(p=0.2, inplace=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (fcn): BasicMLP(
    (net): Sequential(
      (0): Sequential(
        (0): Linear(in_features=32, out_features=2, bias=True)
      )
    )
  )
))
[2023-02-22 19:56:46,515][root][INFO] - Run training on fold: 0
[2023-02-22 19:56:48,051][HYDRA] 	#1 : +model=baseGNN model.data_type=graph model.num_layers=2 model.hidden_dim=32 dataset.pt_thr=null train.epochs=20 train.scheduler=null
Error executing job with overrides: ['+model=baseGNN', 'model.data_type=graph', 'model.num_layers=1', 'model.hidden_dim=32', 'dataset.pt_thr=null', 'train.epochs=20', 'train.scheduler=null']
Traceback (most recent call last):
  File "/mnt/workspace/graph_NN/neurograph/neurograph/train/__main__.py", line 63, in main
    metrics = train(ds, cfg)
  File "/mnt/workspace/graph_NN/neurograph/neurograph/train/train.py", line 60, in train
    valid_metrics, best_model = train_one_split(
  File "/mnt/workspace/graph_NN/neurograph/neurograph/train/train.py", line 141, in train_one_split
    out = model(data)
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/workspace/graph_NN/neurograph/neurograph/models/gnn_base.py", line 124, in forward
    z = conv(z, edge_index)
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/tmp/root_pyg/tmpwc4k7ybp.py", line 17, in forward
    x = self.module_0(x, edge_index)
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py", line 182, in forward
    x = self.lin(x)
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py", line 109, in forward
    return F.linear(x, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2023-02-22 19:56:48,238][root][INFO] - Config:
model:
  name: baseGNN
  n_classes: 2
  data_type: graph
  num_layers: 2
  layer_module: GCNConv
  hidden_dim: 32
  use_abs_weight: true
  use_weighted_edges: false
  final_node_dim: 32
  pooling: mean
  dropout: 0.2
  use_batchnorm: true
  num_heads: 2
  mlp_config:
    in_size: null
    out_size: null
    act_func: null
    act_func_params: null
    layers: []
seed: 1380
dataset:
  name: cobre
  data_type: graph
  experiment_type: fmri
  atlas: aal
  data_path: !!python/object/apply:pathlib.PosixPath
  - /
  - mnt
  - workspace
  - graph_NN
  - neurograph
  - datasets
  abs_thr: null
  pt_thr: null
  feature_type: conn_profile
train:
  device: cuda
  epochs: 20
  batch_size: 8
  valid_batch_size: 8
  optim: Adam
  optim_args:
    lr: 0.001
    weight_decay: 0.0001
  scheduler: null
  scheduler_metric: loss
  scheduler_args:
    factor: 0.1
    patience: 5
    verbose: true
  select_best_metric: loss
  loss: CrossEntropyLoss
  loss_args:
    reduction: sum
  prob_thr: 0.5
log:
  test_step: 1
  wandb_project: mri_gnn_2
  wandb_name: null
  wandb_mode: null
[2023-02-22 19:56:48,298][root][INFO] - Model architecture:
baseGNN(
  (convs): ModuleList(
    (0): Sequential(
      (0): GCNConv(116, 32)
      (1): LeakyReLU(negative_slope=0.2)
      (2): Dropout(p=0.2, inplace=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): Sequential(
      (0): GCNConv(32, 32)
      (1): LeakyReLU(negative_slope=0.2)
      (2): Dropout(p=0.2, inplace=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (fcn): BasicMLP(
    (net): Sequential(
      (0): Sequential(
        (0): Linear(in_features=32, out_features=2, bias=True)
      )
    )
  )
))
[2023-02-22 19:56:48,299][root][INFO] - Run training on fold: 0